{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "with gzip.open(\"res/go.json.gz\") as f:\n",
    "    ontology = json.load(f)\n",
    "\n",
    "parents = {}  # { term : list_of_parent_terms }\n",
    "for edge in ontology[\"graphs\"][0][\"edges\"]:\n",
    "    # select only is_a edges\n",
    "    if edge[\"pred\"] == \"is_a\":\n",
    "        parents.setdefault(edge[\"sub\"].split(\"_\")[1], []).append(edge[\"obj\"].split(\"_\")[1])\n",
    "\n",
    "nodes = []  # list of terms\n",
    "labels = {}  # { term : definition }\n",
    "for node in ontology[\"graphs\"][0][\"nodes\"]:\n",
    "    # exclude obsolete terms\n",
    "    if \"GO_\" in node[\"id\"] and \"deprecated\" not in node[\"meta\"]:\n",
    "        nodes.append(node[\"id\"].split(\"_\")[1])\n",
    "        labels[node[\"id\"].split(\"_\")[1]] = node[\"lbl\"]\n",
    "\n",
    "# Build an ancestors dictionary\n",
    "ancestors = {}  # { term : list_of_ancestor_terms }\n",
    "for node in nodes:\n",
    "    node_ancestors = []\n",
    "    node_parents = parents.get(node)\n",
    "    # Loop parent levels until no more parents\n",
    "    while node_parents:\n",
    "        node_ancestors.extend(node_parents)\n",
    "        # Get the parents of current parents (1 level up)\n",
    "        node_parents = [term for parent in node_parents for term in parents.get(parent, [])]\n",
    "    ancestors[node] = node_ancestors\n",
    "\n",
    "\n",
    "# *** Build a dictionary for the children (similar to the ancestors one) efficiently\n",
    "children = {}  # { node : list_of_children }, leaf terms are not keys\n",
    "for node in ancestors:\n",
    "    for ancestor in ancestors[node]:\n",
    "        children.setdefault(ancestor, set()).add(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    dataset = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(line[:-1])\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def gen_block(f):\n",
    "    \"\"\"\n",
    "    Parse and split the input.\n",
    "    The input must be sorted by target name, second column.\n",
    "\n",
    "    UniProtKB       A0A024R1R8      hCG_2014768             GO:0002181      PMID:21873635   IBA     PANTHER:PTN002008372|SGD:S000007246     P       HCG2014768, isoform CRA_a       hCG_2014768     protein taxon:9606      20171102        GO_Central\n",
    "    UniProtKB       A0A024RBG1      NUDT4B          GO:0003723      GO_REF:0000037  IEA     UniProtKB-KW:KW-0694    F       Diphosphoinositol polyphosphate phosphohydrolase NUDT4B NUDT4B  protein taxon:9606      20191109        UniProt\n",
    "    UniProtKB       A0A024RBG1      NUDT4B          GO:0005829      GO_REF:0000052  IDA             C       Diphosphoinositol polyphosphate phosphohydrolase NUDT4B NUDT4B  protein taxon:9606      20161204        HPA\n",
    "    \"\"\"\n",
    "    name, old_name = None, None\n",
    "    chunk = []\n",
    "    for line in f:\n",
    "        line = line.decode()\n",
    "        if line and line[0] != \"!\":\n",
    "            _, name, _, _, term, _, ec, _, namespace, protein_name = line.split(\"\\t\")[:10]\n",
    "            term = term[3:]  # remove \"GO:\" from the term ID\n",
    "            if name != old_name and old_name:\n",
    "                yield old_name, set(chunk)  # return a set as there can be repetitions, i.e. the same term with different evidence codes\n",
    "                chunk = []\n",
    "            old_name = name\n",
    "            chunk.append(term)\n",
    "    # Last line\n",
    "    if old_name:\n",
    "        yield old_name, set(chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def GO_counts(path, dataset=None):\n",
    "    if path:\n",
    "        dataset = get_dataset(path)\n",
    "    term_counts = {}\n",
    "    with gzip.open(\"res/goa_human.gaf.gz\") as f:\n",
    "        for acc, annotations in gen_block(f):\n",
    "            if acc in dataset: # if the protein is in the dataset passed to the function\n",
    "                for term in annotations:\n",
    "                    term_counts.setdefault(term, 0)\n",
    "                    term_counts[term] += 1\n",
    "    return term_counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def enrichment(domains, dataset):\n",
    "    go_terms_counter_dataset: dict = GO_counts(None, dataset)\n",
    "    num_go_dataset = sum(go_terms_counter_dataset.values())\n",
    "    \n",
    "    go_terms_counter_human = GO_counts(\"datasets/original.txt\")\n",
    "    num_go_human = sum(go_terms_counter_human.values())\n",
    "    \n",
    "    \n",
    "    key_intersection = set(go_terms_counter_dataset.keys()).intersection(go_terms_counter_human.keys())\n",
    "    \n",
    "    # Measure enrichment performing a Fisherâ€™s exact test (hypergeometric test).\n",
    "    \n",
    "    from scipy.stats import fisher_exact\n",
    "    \n",
    "    def p_value(go_terms_counter_dataset, num_go_dataset, go_terms_counter_human, num_go_human):\n",
    "        p_values = {}\n",
    "        for key in key_intersection:\n",
    "            n_term = go_terms_counter_dataset[key]\n",
    "            human_num = go_terms_counter_human[key]\n",
    "            not_dataset = num_go_dataset - n_term\n",
    "            not_human = num_go_human - human_num\n",
    "            p_value = fisher_exact([[n_term, human_num],[not_dataset, not_human]])[1]\n",
    "            p_values.setdefault(key, p_value)\n",
    "        return p_values\n",
    "    \n",
    "    \n",
    "    go_p_values = p_value(go_terms_counter_dataset, num_go_dataset, go_terms_counter_human, num_go_human)\n",
    "    go_p_values = {k: v for k, v in sorted(go_p_values.items(), key=lambda item: item[1], reverse=False)}\n",
    "    go_p_values = dict(filter(lambda v: v[1] < 0.05, go_p_values.items()))\n",
    "    \n",
    "    \n",
    "    # *** Calculate the minimum depth (distance from the root) of each term\n",
    "    def calc_depth():\n",
    "        roots = set(nodes) - set(parents.keys())\n",
    "        depth = {}  # { term : min_depth }\n",
    "        for node in nodes:\n",
    "            c = 0  # Depth level\n",
    "            node_parents = parents.get(node)\n",
    "            while node_parents:\n",
    "                c += 1\n",
    "                if roots.intersection(set(node_parents)):  # break the loop if the root is among parents\n",
    "                    break\n",
    "                # Get the parents of current parents (1 level up)\n",
    "                node_parents = [term for parent in node_parents for term in parents.get(parent, [])]\n",
    "            depth[node] = c\n",
    "        return depth\n",
    "    \n",
    "    depths = calc_depth()\n",
    "    \n",
    "    \n",
    "    # Take into consideration the hierarchical structure of the ontologies and report only most enriched branches, i.e. high level terms.\n",
    "    \n",
    "    import numpy as np\n",
    "    for key in go_p_values.keys():\n",
    "        go_p_values[key] = (go_p_values[key], depths[key])\n",
    "    \n",
    "    mean = np.mean([v[1] for v in go_p_values.values()])\n",
    "    \n",
    "    go_p_values = dict(filter(lambda v: v[1][1] <= 5, go_p_values.items()))\n",
    "    \n",
    "    \n",
    "    go_labeled_enriched = {}\n",
    "    for k, v in go_p_values.items():\n",
    "        go_labeled_enriched.setdefault(labels[k], int(np.log(1 / v[0])))\n",
    "    \n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    wordcloud = WordCloud(max_font_size=200, max_words=289, width=3000, height=3000, background_color=\"white\", scale=4)\n",
    "    wordcloud.generate_from_frequencies(go_labeled_enriched)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.xticks([],[])\n",
    "    plt.yticks([],[])\n",
    "    plt.savefig('plot/wordcloud_architecture_{}.pdf'.format(domains))\n",
    "    plt.axis(\"off\")\n",
    "    plt.close()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# some JSON:\n",
    "with open('datasets/architecture_datasets.json') as json_file:\n",
    "    data : dict= json.loads(json_file.read())\n",
    "\n",
    "for k, v in data.items():\n",
    "    k = k.replace(', ', '\\\\')\n",
    "    k = k.replace(' ', '_')\n",
    "    enrichment(k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}